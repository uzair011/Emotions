# ðŸŽ¯ emotion-aware-core

A modular framework for **real-time emotion recognition** via facial expressions and speech input.

---

## ðŸ“š Table of Contents

- [About](#-about)
- [Features](#-features)
- [Installation](#-installation)
- [Usage](#-usage)
- [Customization](#-customization)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)
- [Links](#-links)

---

## ðŸ” About

`emotion-aware-core` is an open-source, modular framework designed to detect human emotions in real-time using **multimodal inputs**â€”facial expressions and speech. This system acts as an emotional intelligence layer, adaptable across various domains:

- ðŸŽ§ **Customer service** â€” escalate calls when frustration is detected
- ðŸ§  **Mental health apps** â€” monitor long-term mood changes
- ðŸ›¡ï¸ **Security systems** â€” flag suspicious emotional behavior
- ðŸ“š **Education tools** â€” track engagement levels during learning

**Key Philosophy**:
- ðŸ§© **Modular**: Swap models and detectors without altering the pipeline
- âš¡ **Real-Time**: Optimized for consumer-grade CPUs/GPUs (~12 FPS)
- ðŸ§ª **Ethical AI**: Built with bias-awareness and transparency in mind

**Tech Stack**:
- ðŸŽ¥ Visual: `ResNet-18` (FER2013) + `Haar Cascades`
- ðŸŽ™ï¸ Audio: `1D-CNN` (RAVDESS) + `Librosa`
- ðŸ”€ Fusion: Weighted averaging (Visual 0.6 / Audio 0.4)

---

## ðŸš€ Features

- â±ï¸ Real-time emotion detection from webcam and microphone
- ðŸ”„ Modular design for easy integration and upgrades
- ðŸ› ï¸ Prebuilt demos: CLI, webcam inference, and REST API
- ðŸ’» Cross-platform support (Linux, macOS, Windows)

---

## ðŸ“¥ Installation

### Prerequisites
- Python 3.8+
- pip

### Steps

**1. Clone the repository**
```bash
git clone https://github.com/your-username/emotion-aware-core.git
cd emotion-aware-core
```

2. Install dependencies
`pip install -r requirements.txt`

3. Download pretrained models
 ```
# Download ResNet-18 and 1D-CNN model weights
wget https://your-model-hosting.com/models/resnet18_weights.pth -P models/
wget https://your-model-hosting.com/models/1dcnn_weights.h5 -P models/
```

## ðŸ–¥ï¸ Usage
1. Real-Time Webcam Demo
`python scripts/multimodel/realtime_inference.py`

Output:

- Webcam window with predicted emotion overlays
- Terminal logs showing emotion scores and frame latency

## 2. CLI Tools
Image Inference:  
`python scripts/visual/inference_resnet18.py --image path/to/image.jpg`

Audio Inference:
`python scripts/audio/inference_1dcnn.py --audio path/to/audio.wav`

## 3. API Integration
```
from core.emotion_pipeline import EmotionEngine

engine = EmotionEngine()
emotion = engine.predict(frame, audio_clip)
```

## ðŸ¤ Contributing
We welcome your ideas and code! Hereâ€™s how to get started:

- Fork the repository
- Create a branch

`git checkout -b feature/your-feature`

- Commit changes
`git commit -m "Add your feature"`

- Push and open a pull request

## ðŸ“œ License
This project is licensed under the MIT License.

## ðŸ”— Links
FER2013 Dataset => https://www.kaggle.com/datasets/msambare/fer2013
RAVDESS Dataset => https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio
